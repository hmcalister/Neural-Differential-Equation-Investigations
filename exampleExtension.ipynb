{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e56bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f3f4fa",
   "metadata": {},
   "source": [
    "# Constants and Parameters\n",
    "\n",
    "Note that many of these parameters will be explicitly ignored / changed in the following experiments. These simply provide a baseline for future experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9299c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "TORCH_DEVICE = \"cuda\"\n",
    "\n",
    "DATA_STEPS = 1000   # The number of time steps to simulate when creating the data. In practice this is defined by collection of data, not simulation.\n",
    "\n",
    "USE_ADJOINT = True # Defines how to solve the ODE. Adjoint is typically more stable.\n",
    "BATCH_SIZE = 20     # The number of samples to take in each batch. Defines the number of time intervals that are sampled for each batch of learning.\n",
    "BATCH_TIME = 10     # Defines the length of each time interval for learning.\n",
    "NUM_EPOCHS = 2000   # Number of epochs to train over\n",
    "TEST_FREQUENCY = 20 # How many epochs between testing the network during training (mainly for visualization)\n",
    "HIDDEN_LAYER_SIZE = 50  # Defines the (very simple) neural network architecture\n",
    "\n",
    "if USE_ADJOINT:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "else:\n",
    "    from torchdiffeq import odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f7587",
   "metadata": {},
   "source": [
    "# Initial Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0622483c",
   "metadata": {},
   "source": [
    "### Differential Equation Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce1673",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_y0 = torch.tensor([[2., 0.]]).to(TORCH_DEVICE) # Define the true initial condition for the simulation. In practice, defined by data collection.\n",
    "t = torch.linspace(0., 25., DATA_STEPS).to(TORCH_DEVICE)    # Define the time steps starting from the true_y0\n",
    "true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]]).to(TORCH_DEVICE) # Define the true matrix determining the differential equation. THIS IS WHAT WE ARE MODELLING!!!\n",
    "\n",
    "# Define the differential equation of the system as a nn.Module for future integration in torch.\n",
    "# \n",
    "# Interpret Lambda as dy/dt = f(t ,y) such that forward implements f(t, y)\n",
    "class Lambda(nn.Module):\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(y**3, true_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f480bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method=\"dopri5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78578aed",
   "metadata": {},
   "source": [
    "### Neural Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40240558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of data to train the neural network on\n",
    "def get_batch():\n",
    "    # First get a bunch of random start time indices. Note that we allow enough room for a full batch (defined by BATCH_TIME) at each choice.\n",
    "    # s.shape = (BATCH_SIZE).\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(DATA_STEPS - BATCH_TIME, dtype=np.int64), BATCH_SIZE, replace=False))\n",
    "    \n",
    "    # Define the initial conditions for each start time choice.\n",
    "    # batch_y0.shape = (BATCH_SIZE, *dimension)\n",
    "    batch_y0 = true_y[s]\n",
    "\n",
    "    # Get the time for the batch. Note that we use the same time frame for each initial condition, \n",
    "    # i.e. we are reframing the problem to start from t = 0. This is fine, just a shift of the time variable (?)\n",
    "    # batch_t.shape = (BATCH_TIME)\n",
    "    batch_t = t[:BATCH_TIME]\n",
    "\n",
    "    # Get the true value of y for the batch. Collect the right intervals from true_y based on start time and BATCH_TIME.\n",
    "    # batch_y.shape = (BATCH_TIME, BATCH_SIZE, *dimension)\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(BATCH_TIME)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(TORCH_DEVICE), batch_t.to(TORCH_DEVICE), batch_y.to(TORCH_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce7a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network that will learn the equation and dynamics\n",
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    # Set up architecture just like any \"normal\" network\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        # Input is defined by the number of measurements fed in \n",
    "        # e.g. [y**1, y**2, y**3, sin(y)] would be (4, *dimension) for y.shape = dimension\n",
    "        # \n",
    "        # Output is simply y.shape = dimension\n",
    "        # \n",
    "        # Hidden layer (may be many, but only one is required, and easy to interpret) can be small\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, HIDDEN_LAYER_SIZE),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(HIDDEN_LAYER_SIZE, 2),\n",
    "        )\n",
    "\n",
    "        # Initialization... is this useful? Required?\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    # Define forward based on the differential equation. If we *know* dy/dt only depends on the cube, this is fine... but how robust?\n",
    "    def forward(self, t, y):\n",
    "        return self.net(y**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9168a",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdcf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = ODEFunc().to(TORCH_DEVICE)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3) # Strange Optimizer... any reason for this choice?\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(TORCH_DEVICE) # See that we solve the ODE in the same way as simulation... but now use NN not simulation.\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y)) # Loss is MAE? What about MSE?\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print(f\"Epoch {epoch:04d} | Total Loss {loss.item():.6f}\", end=\"\\r\")\n",
    "print()\n",
    "with torch.no_grad():\n",
    "    pred_A = func.forward(0, torch.eye(2).to(TORCH_DEVICE))\n",
    "print(f\"TRUE A:\\n{true_A}\")\n",
    "print(f\"PRED A:\\n{pred_A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537f1c7d",
   "metadata": {},
   "source": [
    "# Overparameterize Network Inputs\n",
    "\n",
    "What if we give the network *more* than just y**3? What if we present, say, the first five powers of y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91fe66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPONENT_COUNT = 5   # The number of powers of y to present. For example, DIMENSION = 3 corresponds to [y**1, y**2, y**3]\n",
    "exponentVector = torch.arange(1,EXPONENT_COUNT+1).repeat_interleave(2).to(TORCH_DEVICE) # Note that the 2 comes from the dimension of y being 2\n",
    "\n",
    "true_y0 = torch.tensor([[2., 0.]]).to(TORCH_DEVICE)\n",
    "t = torch.linspace(0., 25., DATA_STEPS).to(TORCH_DEVICE)\n",
    "# Note that now the inputs to the differential equation / network is represented by a vector corresponding to powers.\n",
    "# For y = (y1, y2), the input is now (y1, y2, y1**2, y2**2, y1**3. y2**3, ...)\n",
    "# So to encode the same example as above, start with an empty coefficient matrix and set the correct coefficients\n",
    "true_A = torch.zeros(size=(2*EXPONENT_COUNT, 2)).to(TORCH_DEVICE)\n",
    "true_A[4,:] = torch.tensor([-0.1, 2.0])\n",
    "true_A[5,:] = torch.tensor([-2.0, -0.1])\n",
    "\n",
    "# Here is a sanity check: \n",
    "# - the shape of y at any time (i.e. what our differential equation outputs) is (1, 2)\n",
    "# - the shape of our network inputs (the powers of y) is (1, 2*POWERS)\n",
    "# - the shape of our coefficient matrix A is (2*POWERS, 2)\n",
    "# Hence, the multiplication of the tensor of inputs and the coefficient matrix will give the correct output\n",
    "print(true_y0.shape, torch.pow(true_y0.tile(EXPONENT_COUNT), exponentVector).shape, true_A.shape)\n",
    "\n",
    "# We update Lambda to use the powers of y in the forward pass\n",
    "class Lambda(nn.Module):\n",
    "    \n",
    "    def forward(self, t, y):\n",
    "        return torch.mm(torch.pow(y.tile(EXPONENT_COUNT), exponentVector), true_A)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    true_y = odeint(Lambda(), true_y0, t, method=\"dopri5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ed25bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching remains the same\n",
    "\n",
    "def get_batch():\n",
    "    s = torch.from_numpy(np.random.choice(np.arange(DATA_STEPS - BATCH_TIME, dtype=np.int64), BATCH_SIZE, replace=False))\n",
    "    batch_y0 = true_y[s]\n",
    "    batch_t = t[:BATCH_TIME]\n",
    "    batch_y = torch.stack([true_y[s + i] for i in range(BATCH_TIME)], dim=0)  # (T, M, D)\n",
    "    return batch_y0.to(TORCH_DEVICE), batch_t.to(TORCH_DEVICE), batch_y.to(TORCH_DEVICE)\n",
    "\n",
    "\n",
    "# But network must now accept the correct number of inputs\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2*EXPONENT_COUNT, HIDDEN_LAYER_SIZE),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(HIDDEN_LAYER_SIZE, 2),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        return self.net(torch.pow(y.tile(EXPONENT_COUNT), exponentVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = ODEFunc().to(TORCH_DEVICE)\n",
    "optimizer = optim.RMSprop(func.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    optimizer.zero_grad()\n",
    "    batch_y0, batch_t, batch_y = get_batch()\n",
    "    pred_y = odeint(func, batch_y0, batch_t).to(TORCH_DEVICE)\n",
    "    loss = torch.mean(torch.abs(pred_y - batch_y))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % TEST_FREQUENCY == 0:\n",
    "        with torch.no_grad():\n",
    "            pred_y = odeint(func, true_y0, t)\n",
    "            loss = torch.mean(torch.abs(pred_y - true_y))\n",
    "            print(f\"Epoch {epoch:04d} | Total Loss {loss.item():.6f}\", end=\"\\r\")\n",
    "print()\n",
    "with torch.no_grad():\n",
    "    pred_A = func.net(torch.eye(2*EXPONENT_COUNT).to(TORCH_DEVICE))\n",
    "print(f\"TRUE A:\\n{true_A}\")\n",
    "print(f\"PRED A:\\n{pred_A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1edeed2",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 4), facecolor='white')\n",
    "ax_traj = fig.add_subplot(131, frameon=False)\n",
    "ax_phase = fig.add_subplot(132, frameon=False)\n",
    "ax_vecfield = fig.add_subplot(133, frameon=False)\n",
    "\n",
    "ax_traj.set_title('Trajectories')\n",
    "ax_traj.set_xlabel('t')\n",
    "ax_traj.set_ylabel('x,y')\n",
    "ax_traj.plot(t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 0], t.cpu().numpy(), true_y.cpu().numpy()[:, 0, 1], 'g-', label=\"True\")\n",
    "ax_traj.plot(t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 0], '--', t.cpu().numpy(), pred_y.cpu().numpy()[:, 0, 1], 'b--', label=\"Pred\")\n",
    "ax_traj.set_xlim(t.cpu().min(), t.cpu().max())\n",
    "ax_traj.set_ylim(-2, 2)\n",
    "ax_traj.legend()\n",
    "\n",
    "ax_phase.set_title('Phase Portrait')\n",
    "ax_phase.set_xlabel('x')\n",
    "ax_phase.set_ylabel('y')\n",
    "ax_phase.plot(true_y.cpu().numpy()[:, 0, 0], true_y.cpu().numpy()[:, 0, 1], 'g-')\n",
    "ax_phase.plot(pred_y.cpu().numpy()[:, 0, 0], pred_y.cpu().numpy()[:, 0, 1], 'b--')\n",
    "ax_phase.set_xlim(-2, 2)\n",
    "ax_phase.set_ylim(-2, 2)\n",
    "\n",
    "ax_vecfield.set_title('Learned Vector Field')\n",
    "ax_vecfield.set_xlabel('x')\n",
    "ax_vecfield.set_ylabel('y')\n",
    "\n",
    "y, x = np.mgrid[-2:2:21j, -2:2:21j]\n",
    "dydt = func(0, torch.Tensor(np.stack([x, y], -1).reshape(21 * 21, 2)).to(TORCH_DEVICE)).cpu().detach().numpy()\n",
    "mag = np.sqrt(dydt[:, 0]**2 + dydt[:, 1]**2).reshape(-1, 1)\n",
    "dydt = (dydt / mag)\n",
    "dydt = dydt.reshape(21, 21, 2)\n",
    "\n",
    "ax_vecfield.streamplot(x, y, dydt[:, :, 0], dydt[:, :, 1], color=\"black\")\n",
    "ax_vecfield.set_xlim(-2, 2)\n",
    "ax_vecfield.set_ylim(-2, 2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
